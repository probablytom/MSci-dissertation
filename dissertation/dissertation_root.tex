\input{preamble}

\begin{document}

\title{Investigating Computational Responsibility}
\author{William Wallis}
\matricnum{2025138}

\maketitle

\begin{abstract}
    Currently, models are produced for responsibility modelling which have their roots in logic. These models, while sophisticated, suffer from a lack of pragmatism: for guiding agent behaviour in sociotechnical simulations, logical models are not always ideal. In the similar field of trust modelling, algorithmic models which emulate social behaviour produce useful results while being easier to understand, implement, and reason about. In this paper\todo{paper? report? project?}, a proof-of-concept responsibility modelling platform adopting the algorithmic formalism style employed by trust modelling is produced, and its utility evaluated.
\end{abstract}

\section{Introduction}
A growing area of research lies in the formalism of human traits into computational representations. These algorithms make computers more human-like; for that reason, they are referred to here as ``Anthropomorphic Algorithms''\todo{improve the introduction of the Anthropomorphic Algorithms term}. A similar term, ``human-like computing'', has also risen in popularity lately. Human-like computing does not strictly focus on the implementation of formalisms of human traits, however, which is the area of interest for this report.\par

This implementation interest, realised in the study of anthropomorphic algorithms, presents an interesting sociotechnical problem. They present an opportunity to alter the behaviour of actors in a sociotechnical system, and to do so in a way that is easy to reason about. This alternation of behaviour is done by the algorithmic implementation of a \emph{formalism}. Formalisms present a concrete definition --- by process, mathematical definition or semantic description --- which can be used to construct an anthropomorphic algorithm. These formalisms tend to attempt to model in one of two ways:

\begin{enumerate}
    \item Modelling the trait as a useful metaphor\\
    These models tend to be inaccurate with regards the social science surrounding the trait that they model. However, they make a trade-off between this accuracy and the model's utility. For example, the notion of trust as a metaphor for a type of behaviour might be useful in information security research, but what matters in the formalisms implemented for this research is the formalism's utility in information security --- \emph{not} whether the formalism accurately represents human trust.
    \item Modelling social science directly\\
    These models attempt to accurately model the traits they concern. This can be useful for fields such as sociotechnical modelling, as well as social sciences research. There are also interesting applications for these models in interaction study: making interfaces interact with users in a human-like way, and representing the states of these traits to the users, are valuable research areas which are more applicable to these type-2 formalisms than to type-1 formalisms.
\end{enumerate}

In reality, most formalisms and their implementations lie somewhere on the spectrum that these two types define.

\section{Statement of Problem}
Computational formalisms of human traits are a growing field of research, with applications in lots of different areas. A problem with these anthropomorphic algorithms is that there is limited breadth to the scope of existing research in the field (as is demonstrated during the background survey in \cref{sec:related_work}). The metaphor of the human trait in these algorithms remains largely unexplored.\par

Breadth in the application of the metaphor is important, however. The importance stems from the utility in the human metaphor when designing systems:

\begin{itemize}
  \item Human-Computer Interaction can make use of behavioural metaphors to relay complicated internal states to a user. Storer et al.~\cite{storer_mobile_behaviour_poster} demonstrated methods by which a mobile device might dissuade certain user actions by expressing its ``discomfort'' or lack of ``trust'' in its interaction design.
  \item Information Security can make use of behavioural metaphors in order to increase difficulty of access when negative system states are encountered. A system might allow access on a graded scale, dependant on internal states of trust, comfort, and confidence.
  \item Theoretical advancements in smart city technology\cite{wallis_talk_about_x_talk} might increase a city's resilience by integrating notions of responsibility into public services and the environment on a community scale.\par
\end{itemize}

While similar results can often be achieved using regular techniques, the human metaphor allows for a better communication between a human user and complicated system states. All of the above examples center around this notion; however, the applications extend beyond Human-Computer Interaction research.\par

The lack of application of the human metaphor is a complicated mosaic of related factors. For example, research into anthropomorphic algorithms holds particular challenges, as a result of its strongly interdisciplinary nature: it requires a research team to understand the nuances of sciences as well as social sciences, and sometimes even humanities. Not only does the research team require the ability to understand these nuances, but the research must take into account their different natures. This often causes divergence in the philosophies of sociotechnical research. Some researchers view sociotechnical systems from the perspective of largely human-based systems with abstract, social behaviour. Others see sociotechnical systems as a combination of dynamic, mathematical processes which produce more technical emergent phenomena. This hints at a third complicating factor, (the second being a lack of convergence in research focus): a lack of a consistent modelling paradigm. Some research focuses largely on actor interaction-style modelling techniques (as in \cite{baxter2011socio}), while others rely on purely graphical modelling (as in \cite{ObashiMethodology}), or on mathematical modelling techniques (as in \cite{vespignani2012modelling}). \par

These issues together pose an issue for research in anthropomorphic algorithms: a formalism of a human-like trait is only useful to certain researchers, for certain types of models, with certain sociotechnical philosophies. Their lack of broad application is therefore unsurprising; these factors compound to produce yet another, which is that the breadth of traits formalised and researched is very small. The largest degree of research is easily conducted in the field of Trust; other traits, such as Comfort, have recently been attempted also, notably in \cite{marsh2011defining}.

Recently, some interest has been shown in research pertaining to modelling and formalising \emph{responsibility}. In a way, this is to be expected: responsibility could be considered to be the analysis of an agent's performance, as trust often is also, and in this way the traits are very similar. In particular, \cite{CastelfranchiSocialApproach}~'s logical model of social trust could be turned into a proof-of-concept formalism of responsibility with only a small addition: adding an obliging term in a similar way to the Deontic Logic's system for obliging (discussed in \cite{deontic-logic}), allowing an agent to effectively delegate a task to another which is deemed responsible in discharging responsibilities --- the agent selected being known to be trusted already, by C\&F's already established work. The scheduling of tasks based on trust is a simple extension of existing models, or an application of existing models.\par

A model of responsibility might be more than simple task allocation, however. In particular, a model of responsibility might allow a responsible sociotechnical agent to choose responsibilities to discharge, rather than blindly executing tasks they are provided with.\par

This has a number of potential applications. One such possibility would be to implement agent awareness of remote task execution via RPC. Should an agent on a network be given a procedure to execute which is percieved through a responsibility formalism to be unusual, the procedure may not be executed, or may be rejected upon reciept by the responsible agent. Similar applications have proven effective in trust literature, particuarly the Eigentrust algorithm as laid out in \cite{}, where information security by inferring agent trustworthiness is evaluated.

\section{Related Work}\label{sec:related_work}


\subsection{Trust Modelling}

\subsubsection{Castelfranchi \& Falcone}

\subsubsection{Marsh}

\subsubsection{Eigentrust}


\subsection{Sociotechnical Systems}

\subsubsection{FIRE}

\subsubsection{Ian Sommerville}


\subsection{Philosophical Literature}  % The nature of responsibility, and the validity of modelling anthropic traits in a machine.

\subsubsection{P.F. Strawson}  % The nature of responsibility

\subsubsection{Thomas Scanlon}  % A philosophical perspective which converges on Sommerville's ideas in Dependable Systems

\subsubsection{Sloman}  % The space of artificial minds, and a brief note on the philosophical value of this formalism (optional!)



\section{A Formalism of Responsibility}

\subsection{Design Decisions and Philosophy}

\subsection{Feature 1: Acting on Responsibilities}

\subsection{Feature 2: Judgement of Responsibility}


\section{Evaluating the Formalism}


\section{Future Work}

\subsection{Advancing the formalism}


\section{Discussion}


\bibliographystyle{abbrv}
\bibliography{biblio}

\end{document}

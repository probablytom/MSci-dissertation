\section{Statement of Problem}\label{sec:statement_of_problem}

With some background exploration on what a formalism of responsibility might entail, and an overview of its scope and utility, we can see that some formalism of responsibility has genuine utility. However, assessing how it might apply to artificial agents in practice requires the development of the formalism itself. It also requires that the formalism be applied to the specific category of minds outlines in \cref{subsec:types-of-agents}: reflective, interpretive, reactive agents. \par

We can address the feasibility of a real formalism which applies to these agents --- and develop said formalism in the process --- by answering the following research questions:

\input{research-questions}

I propose that this work would provide a valuable addition to the development of anthropomorphic trait formalisms, and that the work is also useful and interesting in its own right. These questions were also chosen in particular for a few reasons, which are detailed here. \par

\subsection{Research by Scientific Method}

A desired property of the questions chosen was that they should not be answerable by general insight from relevant literature. Instead, they should be answerable only by experimental method. This is because research questions which are answerable by literature insight are:
\begin{enumerate}[label=\emph{\Alph*}:]
    \item Best solved by a literature review, rather than scientific experiment, as where literature analysis is a viable research option it can be much more cost and time effective.
    \item Inappropriate for masters level research and therefore unfit for the project at hand.
\end{enumerate}

Therefore, research questions answerable only by experimental method are required, as experiment-driven research is the most appropriate research for a masters project, and questions answerable by literature review should in general be answered by this method (though there are of course exceptions).\par

The questions selected fit this criteria. This is because a responsibility formalism's efficacy in directing agent decisions and assisting an agent in assessing the responsibility of other agents is unclear upon the construction of the formalism. As will be explored, when the proposed formalism is fully defined, experiments must be undertaken in order to tweak the formalism's structure so as to effectively answer the questions --- experimental data is required to determine whether agent decisions are suitably swayed and algorithms can properly assess other agents' responsibility. Therefore, the questions provided meet the requirement for experimental exploration.\par

\subsection{Appropriately Fit Agent Context}
In \cref{subsec:types-of-agents}, an argument was presented for certain traits agents must exhibit in order for applying a responsibility formalism to their behaviour to make sense. Appropriate research questions to be undertaken in the development of such a formalism should therefore limit the types of agents they apply to, such that those agents meet those criteria.\par

It can be seen that the research questions proposed limit the scope of the agents a formalism would apply to in just this way. For example, to direct the decisions made by an intelligent agent, that agent must make decisions taking the formalism into account --- its decision function is therefore parametrised by the agent's responsibility. This makes that agent reflective. \par

For an agent to assess the actions of other agents, one would want that agent to be able to come to different conclusions about the actions of other agents than that agent did --- it would therefore have to make use of a subjective interpretation function to come to these analyses. While this interpretation function isn't strictly necessitated by the agents a formalism would apply to --- there are other ways to introduce subjectivity, such as differing training data on learning algorithms --- it suggests development in this direction, as an interpretation function would be a very simple and not very limiting way to introduce this subjectivity. Parametrising this interpretation function by the current environment would also ensure an agent was reflective, making the interpretation function approach particularly appealing and admitting the final trait suggested by the problem context in \cref{subsec:types-of-agents}.\par

Given that the research questions proposed imply the correct types of agents as much as is possible, we will explore these questions in the literature that might help to indicate how to construct a solution to the problem. As experimental data will show whether a formalism will answer these research questions through an analysis of the ``responsibleness'' of their actions, relevant literature should inform the construction of the hypothetical formalism. This will in turn explore what an experiment which answers the proposed questions will look like.\par

% Computational responsibility is a complex area with lots of incidentally related work, but no specific relevant literature. Instead of focusing on the responsibilities of artificial agents, their responsibilities are implied by the construction of the agent itself. It might employ algorithms for driving without human guidance, or classify network traffic in an attempt to flag attempts at a system's security. In these instances, lots of somewhat-related work has been done on computational \emph{trust}: can one artificial agent trust another?\par
% 
% However, simply building ``responsibility'' into a system without any understanding of responsibly-made decisions, or ability to learn obligations and duties in a concrete way, means we lose a great opportunity. Moreover, while trust and responsibility are intrinsically linked social concepts, no work has been done to migrate the models of trust to new models of responsibility that consider topics like obligation and duty. A concern arises: do artificially intelligent agents, which we put at the helm of concerns like network security and road safety, miss out as a result of their failure to consider duty and obligation? Two examples present themselves.\par
% 
% The first: a car might drive along a residential street and identify a squirrel running across the road in front of it. It calculates a high probability that, unless it swerves out of the way of the squirrel, it may kill it. It simultaneously identifies that, in the country it is driving in, the law states that it should swerve to avoid killing animals if possible. Computational responsibility introduces itself into the problem in that the car should also have a social understanding: will the swerve endanger humans? How strongly should it weight that probability into the action it chooses? Is it also responsible for, say, conserving fuel for environmental reasons? And if so, which responsibility is more important? \par
% 
% The key here is that the car has many goals to ascertain; while some are more immediate than others, it should have the capacity to weigh \emph{multiple, arbitrary responsibilities} up to surmise what its next action is. Clearly, this is a problem for decision theory; but one where an understanding of responsibility may be of great help. Unfortunately, this example may be only expository as of yet: a practical model of a self-driving car with this degree of responsible awareness would be rather complex, and a model this advanced is beyond the scope of this project.\par
% 
% The second: two intelligent agents raise or lower the price of a book they manage according to perceived changes in the market. This is common practice on large sites where prices of unusual books can fluctuate according to sudden rises in demand, as seen in \cref{fig:amazon_price_hiking}.\todo{Confirm figure referencing style}
% 
% \begin{figure}[h]
% \label{fig:amazon_price_hiking}
% \centering
% \includegraphics[trim=0.7cm 0.7cm 0.7cm 0.7cm, scale=0.75]{images/amazon_price_hike_fly_genetics}
% \caption{Bots on Amazon artificially inflate a book price to around \emph{62850\%} its used price}
% \end{figure}
% 
% In \cref{fig:amazon_price_hiking}, one artificial agent is known to have artificially inflated the price of a book; another agent has \emph{also} inflated the price according to the seeming market trend. The first agent, seeing that the book is rising in value and now under priced, inflates the price of its own copy, and the cycle continues until a human intervenes.\par
% 
% Kevin Slavin discusses the idea that we have begun to design a world \emph{``for algorithms, with nothing but a big red button, labelled ``stop''~''}\cite{SlavinHOWWORLD}. The precession of this design trend marches on, relentless --- but algorithms, rather than their interfaces, can be built with humans in mind. A mutual understanding of responsibility would allow one algorithm in this cycle to delegate the price inflation of its book to the other, breaking the cycle, so long as the concept of responsibility for a task is mutually understood. This is where the second, real-world example of computational responsibility lies. \par
% 
% As can be seen from the existence of models for concepts such as Trust which can solve similar HCI problems~\cite{designing_with_trust}, mimicking human traits computationally has its benefits. Moreover, we can be certain that, just as with trust modelling, useful and thorough responsibility models can produce work in machine ethics\cite{Moor2006}, sociology\cite{Macy2002}, and clearly, computing science. We can therefore expect that a computational model of responsibility will yield similar results --- perhaps breaking new ground in other fields, which traits like Trust or Comfort have less relevance to.\par

% \subsection{Using the definitions}
% These definitions are useful in that they allow us to begin to see what might compose a responsible agent. We might begin to build a computational model of responsibility which embodies these traits by investigating the following research questions:
% \input{research-questions}
% These are the research questions I seek to answer in the course of this project.
% 
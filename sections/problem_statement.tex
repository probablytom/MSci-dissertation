\section{Statement of Problem}\label{sec:statement_of_problem}

With some background exploration on what a formalism of responsibility might entail, and an overview of its scope and utility, we can see that some formalism of responsibility has genuine utility. However, assessing how it might apply to artificial agents in practice requires the development of the formalism itself. It also requires that the formalism be applied to the specific category of minds outlines in \cref{subsec:types-of-agents}: reflective, interpretive, reactive agents. \par

We can address the feasibility of a real formalism which applies to these agents --- and develop said formalism in the process --- by answering the following research questions:

\input{research-questions}

I propose that this work would provide a valuable addition to the development of anthropomorphic trait formalisms, and that the work is also useful and interesting in its own right.\todo{Finish this section!}

% Computational responsibility is a complex area with lots of incidentally related work, but no specific relevant literature. Instead of focusing on the responsibilities of artificial agents, their responsibilities are implied by the construction of the agent itself. It might employ algorithms for driving without human guidance, or classify network traffic in an attempt to flag attempts at a system's security. In these instances, lots of somewhat-related work has been done on computational \emph{trust}: can one artificial agent trust another?\par
% 
% However, simply building ``responsibility'' into a system without any understanding of responsibly-made decisions, or ability to learn obligations and duties in a concrete way, means we lose a great opportunity. Moreover, while trust and responsibility are intrinsically linked social concepts, no work has been done to migrate the models of trust to new models of responsibility that consider topics like obligation and duty. A concern arises: do artificially intelligent agents, which we put at the helm of concerns like network security and road safety, miss out as a result of their failure to consider duty and obligation? Two examples present themselves.\par
% 
% The first: a car might drive along a residential street and identify a squirrel running across the road in front of it. It calculates a high probability that, unless it swerves out of the way of the squirrel, it may kill it. It simultaneously identifies that, in the country it is driving in, the law states that it should swerve to avoid killing animals if possible. Computational responsibility introduces itself into the problem in that the car should also have a social understanding: will the swerve endanger humans? How strongly should it weight that probability into the action it chooses? Is it also responsible for, say, conserving fuel for environmental reasons? And if so, which responsibility is more important? \par
% 
% The key here is that the car has many goals to ascertain; while some are more immediate than others, it should have the capacity to weigh \emph{multiple, arbitrary responsibilities} up to surmise what its next action is. Clearly, this is a problem for decision theory; but one where an understanding of responsibility may be of great help. Unfortunately, this example may be only expository as of yet: a practical model of a self-driving car with this degree of responsible awareness would be rather complex, and a model this advanced is beyond the scope of this project.\par
% 
% The second: two intelligent agents raise or lower the price of a book they manage according to perceived changes in the market. This is common practice on large sites where prices of unusual books can fluctuate according to sudden rises in demand, as seen in \cref{fig:amazon_price_hiking}.\todo{Confirm figure referencing style}
% 
% \begin{figure}[h]
% \label{fig:amazon_price_hiking}
% \centering
% \includegraphics[trim=0.7cm 0.7cm 0.7cm 0.7cm, scale=0.75]{images/amazon_price_hike_fly_genetics}
% \caption{Bots on Amazon artificially inflate a book price to around \emph{62850\%} its used price}
% \end{figure}
% 
% In \cref{fig:amazon_price_hiking}, one artificial agent is known to have artificially inflated the price of a book; another agent has \emph{also} inflated the price according to the seeming market trend. The first agent, seeing that the book is rising in value and now under priced, inflates the price of its own copy, and the cycle continues until a human intervenes.\par
% 
% Kevin Slavin discusses the idea that we have begun to design a world \emph{``for algorithms, with nothing but a big red button, labelled ``stop''~''}\cite{SlavinHOWWORLD}. The precession of this design trend marches on, relentless --- but algorithms, rather than their interfaces, can be built with humans in mind. A mutual understanding of responsibility would allow one algorithm in this cycle to delegate the price inflation of its book to the other, breaking the cycle, so long as the concept of responsibility for a task is mutually understood. This is where the second, real-world example of computational responsibility lies. \par
% 
% As can be seen from the existence of models for concepts such as Trust which can solve similar HCI problems~\cite{designing_with_trust}, mimicking human traits computationally has its benefits. Moreover, we can be certain that, just as with trust modelling, useful and thorough responsibility models can produce work in machine ethics\cite{Moor2006}, sociology\cite{Macy2002}, and clearly, computing science. We can therefore expect that a computational model of responsibility will yield similar results --- perhaps breaking new ground in other fields, which traits like Trust or Comfort have less relevance to.\par

% \subsection{Using the definitions}
% These definitions are useful in that they allow us to begin to see what might compose a responsible agent. We might begin to build a computational model of responsibility which embodies these traits by investigating the following research questions:
% \input{research-questions}
% These are the research questions I seek to answer in the course of this project.
% 
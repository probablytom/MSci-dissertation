%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background Survey}\label{sec:background_survey}

% What work's been done already? 
Computational trust is a topic with a wealth of literature to draw inspiration from in an attempt to produce similar formalisms of responsibility. Marsh~\cite{Marsh1994FormalisingConcept} draws his own inspiration from literature as early as David Birkhoff's 1930s work on creating an ``Aesthetic Measure'', which was in effect a quantification of aesthetics. Much work on developing similar formalisms has been undertaken by a range of fields since.\par

The success of recent formalisms, such as Marsh's, rest as all scientific work does on the research it serves to advance. As successful work as resulted from the study of social traits in social sciences and mathematics, a sensible approach to the construction of a new formalism would be to start from the same foundational concepts.\par

Unfortunately, not all of this related work will ultimately be of use in developing a formalism of responsibility --- responsibility as a concept differs in important ways from trust. For example, trust is a concept often discussed in terms of an agent's trust toward another: ``Anne didn't trust the contractor'' describes Anne's trusting relationship \emph{toward} another actor. ``The contractor performed their work responsibly'', however, is a description of the contractor's responsibility as a trait --- not as a relationship with Anne. Of course, this is one of a number of examples of the different natures of trust and responsibility. Fields which are not explored by trust might therefore be useful in elucidating some aspects of responsibility which trust-relevant literature may not cover. We find that these aspects are well covered by the field of Moral Responsibility, and similar philosophical work.

% Maths, Psychology and Sociology a-la Marsh
\subsection{Social Sciences and Mathematics}
\subsubsection{Birkhoff's Aesthetic Measure}\label{subsubsec:birkhoff}
One of the earlier formalisms of a human factor\footnote{For the sake of clarification, we define a ``human factor'' as an element of a social or sociotechnical system which arises from human behaviour, such as Trust.}\footnote{Also for the sake of clarifying a sociotechnical system, a sociotechnical system is a system composed of human tendencies and behaviours, such as Trust, alongside technical activity, such as a computer or a steam engine. An example might be a coffee shop: \begin{itemize}
    \item Humans take orders and manage the running of the shop
    \item Technology is responsible for complex activities such as taking payments and forcing steam through coffee at high pressure
\end{itemize} so there are both social and technical actors and behaviours in the ``system'' of a day-to-day coffee shop.} was Birkhoff's definition of Aesthetic Measure\cite{BirkhoffAESTHETICMEASURE}. In it, Birkhoff defines the notion of Aesthetic Measure as a ratio of Order to Complexity:
\[M = \frac{O}{C}\]
Birkhoff's work inadvertently gave rise to the notion that human factors can be represented by mathematical equations and systems. Birkhoff's formalism of aesthetics became popular for a few reasons, but one of particular interest to later Trust modelling work was that Birkhoff put a great degree of effort into backing his work up with psychological theory. In this way, Birkhoff's formalism could be said to be a \emph{psychological} formalism. \par

Later trust modelling work followed in Birkhoff's footsteps here. Indeed, Birkhoff gives a solid foundation for the model-creating method later employed by Marsh\cite{Marsh1994FormalisingConcept} and Castelfranchi \& Falcone, as it is:
\begin{itemize}
    \item Founded on mathematical or logical principles which are \emph{quantifiable}
    \item Heavily inspired and directed by related work in psychology, sociology, and philosophy
\end{itemize}\par

The marriage of social studies with mathematical rigour will be a recurring theme of the work related to Computational Trust.\par

\subsubsection{Deutsch}
Following the quantifiable, mathematical work done by Birkhoff, logical and arithmetic formalisms of human factors followed. One of the earlier and more widely adopted models for Trust came from Deutsch in 1962. 
Deutsch is a psychologist who did swathes of work in the topic of cooperation, touching on Trust during the 60s. \par

Deutsch's formalism of trust wasn't immediately quantifiable, but presented one of the earliest well-defined definitions of trust. To paraphrase Deutsch's formalism in ``Cooperation and Trust: Some Theoretical Notes''\cite{deutsch1962cooperation}:
\begin{itemize}
    \item An actor is presented with a choice between two paths.
    \begin{enumerate}[label=\emph{\Alph*}:]
        \item No change
        \item The actor takes some action, of ambiguous outcome. A possible gain is associated, \emph{P}, and some possible risk is associated, \emph{R}.
    \end{enumerate}
    \item The actor assesses that the outcome of choice \emph{B} relies on the behaviour of another actor.
    \item The actor assesses the action they may take and resolves that the strength of \emph{R}, likelihood of \emph{R} as an outcome, or both are higher than the respective \emph{P} measurements.
    \item The actor is said to be \emph{trusting} if they choose to take path \emph{B}.
\end{itemize}

This formalism introduces some interesting notions. For example, it is unclear as to whether the outcome of choice \emph{B} can rely on the same actor making the decision; can one trust oneself by Deutsch's definition? Another interesting analysis of the implications of Deutsch's model is that it does not rely on the \emph{accurate} measurement of risk and utility, but just its perception --- trust is subjective, and based on the trusting actor's perspective on the world.

Rather than characterising trust by the parties involved, Deutsch's formalism is characterised by \emph{risk and utility}. A simple quantification of Deutsch's formalism could be devised, therefore, where risk and utility are quantified by simple assessments using utility functions and a form of risk analysis. Even so, the outcome of this quantified system is a single bit: trusting or not trusting. This does quantify trust, but only technically speaking, and this quantification is weak in its expressiveness. It gives no remit to suggest that one might trust one person over another, for example, as there are no order-able degrees of trust.

Deutsch offers many different ideas as to why and how trust or trust-like behaviour can come about, however. This list is taken from Marsh 1994\cite{Marsh1994FormalisingConcept}, where explanations of all nine can be found:

\begin{enumerate}
    \item Trust as Despair
    \item Trust as Social Conformity
    \item Trust as Innocence
    \item Trust as Impulsiveness
    \item Trust as Virtue
    \item Trust as Masochism
    \item Trust as Faith
    \item Risk-taking or Gambling
    \item Trust as Confidence
\end{enumerate}

Deutsch's given model above specifically targets formalisation of trust as confidence.

\subsubsection{Luhmann}
Luhmann, a sociologist who also worked in Trust and related fields, had his own take on formalisms of Trust: that trust was a social tool for reducing the complexity of a social system. Specifically, Luhmann sees trust as being a method whereby agents in a social system can reduce their exposure of \emph{risk} to each other. According to Luhmann, ``Trust{\ldots} presupposes a situation of risk.''\cite{luhmann2000familiarity}\par

Luhmann's work is therefore difficult to form quantitative formalisms from, as his thesis stems from a risk analysis perspective, which can be particularly difficult in a sociotechnical system. However, Luhmann's work remains interesting; a formalism of a human factor like trust would be incomplete without considering the properties of individual human actors as well as these properties' emergent effects in the larger sociotechnical space. For small systems, these social-level properties may not present themselves very strongly; however, most human factors are present regardless of the scale of the system being modelled. Therefore, a formalism of a human factor which fails to consider both psychological and sociological aspects cannot be complete. \par

% Modern trust (logics, Marsh and C&F)
\subsection{Modern [Computational] Trust methods}
\subsubsection{Marsh's formalism}
The earliest quantifiable formalism of trust which provides computability, flexibility, and an inspiration from the sociological and psychological work above is that of Stephen Marsh in 1994\cite{Marsh1994FormalisingConcept}. Marsh's work breaks trust up into three core quantifications, where each variable takes some value in the range \({[-1,1)}\):
\begin{enumerate}
    \item Basic Trust \\
    This is the general degree of ``trustingness'' about an agent, or that agent's ordinary inclination to trust.
    \item General Trust \\
    General trust is trust in the context of the agent being trusted. Marsh's original description begins\cite{Marsh1994FormalisingConcept}:
    \begin{displayquote}
        Given two agents, \(x, y \in \mathcal{A}\), to notate '\(x\) trusts \(y\)'  we use: \(T_{x}(y)\). {\ldots}The value represents the amount of trust \(x\) has in \(y\) here.
    \end{displayquote}
    So, General Trust can be seen to be the trust that an agent \(x\) has in \(y\).
    \item Situational Trust \\
    Trust doesn't exist in a vacuum, and the only variable isn't the subject of \(x\)'s trust; \(y\) may have varying degrees of competency in performing an action. Therefore, Situational Trust can be seen to be the trust \(x\) holds that \(y\) can actually perform some task, \safealpha. Marsh helpfully gives the example\cite{Marsh1994FormalisingConcept}:
    \begin{displayquote}
        {\ldots}whilst I may trust my brother to drive me to the airport, I certainly would not trust him to fly the plane!
    \end{displayquote}
\end{enumerate}\par

Marsh's three types of trust are helpful in breaking down what matters when discussing trust --- notions like competency, for example --- as well as establishing a jargon for trust. Often, one might say that a person is ``\emph{trusting}'': Marsh's formalism accounts for concepts like this, but establishes it as a less detailed type of trust, and a type of trust which doesn't account for the action being trusted for, or whether the trusted agent is able to complete the action.\par

Marsh also succeeds in introducing concrete examples of computational formalisms of ordinarily human traits --- here Trust. The key aspect of Marsh's advancement is that it goes one step further than a \emph{quantitative} model, and introduces reinforcement learning algorithms which model how trust \emph{changes}, and not just its current state. As seen when discussing Birkhoff's work (\cref{subsubsec:birkhoff}), quantitative formalisms of human traits like Aesthetics had been studied and achieved long before Marsh's work.\par

Since Marsh's work, many trust models have been developed. A small subset of these are reviewed here; offshoots from this seminal work include REGRET, FIRE, and others.\par

\subsubsection{Castelfranchi \& Falcone}
As it turns out, cognitive computational trust models that already exist are almost but not quite appropriate for modelling responsibility. The C\&F trust model requires only four main ingredients to formulate a cognitive trust model:

\begin{enumerate}
    \item \emph{x}, a truster
    \item \emph{y}, a subject of trust
    \item \emph{g}, a goal of \emph{x}
    \item \emph{\safealpha}, an action of \emph{y}
\end{enumerate}\par

This model gets us close to where we need to be to model responsibility; like responsibility modelling often does, it assumes two agents. There also exists some goal which can be met, which --- to use C\&F terminology --- is \emph{delegated} by \emph{x} to \emph{y}. \emph{Y} can achieve this goal through some action, \emph{\safealpha}. So far, all of this forms the beginning of a foundation for cognitive responsibility; what turns delegation of a task into the consignment of responsibility is that of obligation, and the understanding of obligation. \par

It is evident that trust and responsibility models are, even in the human-like cognitive approach, very similar. However, there are drawbacks which mean that we cannot directly apply C\&F theory to the idea of computational responsibility: it does not represent any degree of obligation or address the specific problem of judging responsibility at all.\par

Nevertheless, this presents an exciting insight into work to be done to produce a formalism of responsibility. Particularly, it is evident that there is at least some technical value in listing the individual components as C\&F do. Their simple, reduced approach implies that with the correct identification of elements of responsibility, our formalism can be similarly simple. It is also encouraging that connections between trust and responsibility modelling seem to readily present themselves. We can therefore expect our formalism to rightly exhibit a similar structure and features.\par

\subsubsection{Eigentrust}\label{subsec:eigentrust}
Unlike the formalisms from Marsh and C\&F, Eigentrust\cite{eigentrust} is a trust formalism built without interdisciplinary work in mind. Instead, Eigentrust's main focus is that of software engineering: it is a formalism with the foremost principle of creating secure trust and reputation systems in a computer network. Indeed, the primary driving force of Eigentrust's mathematics is that of reputation, and it leverages linear algebra concepts together with a simple reputation system to create an effective trust formalism. Core to the reputation framework is that Eigentrust adds 1 to a score for a positive interaction, and -1 for a negative interaction, feeding these scores into trust scores which, through a series of matrices and eigenvalues, forms a global ledger of trust.\par

An intriguing feature of Eigentrust is that very global ledger of trust, shared in a peer-to-peer network. Having a shared knowledge of agents who are trustworthy or untrustworthy in a system acts as an interesting utility in exploring the value of a trust formalism. An example alluded to in the paper is that of downloads: if a download successfully completes from one peer to another, the receiving peer rates their server positively. Similarly, if the download fails, the receiving peer rates the server negatively. An agent seeking a certain download therefore attempts to interact with more trusted peers in the global ledger --- this is a satisfying example of the value of a trust formalism in practical engineering.\par

In this way, Eigentrust creates a formalism that:

\begin{enumerate}[label=\emph{\Alph*}:]
    \item Represents gradations of trust
    \item Allows for an effective distributed trust model
\end{enumerate}

However, it is unfortunately a bad fit for the model required for the proof-of-concept responsibility formalism. One reason for this is that Eigentrust explicitly does not attempt to interpret trust values; they are taken at face-value. As a result, the model wouldn't fit the space of minds of possible actors which was originally suggested as appropriate. The formalism constrains behaviour in other ways for appropriate trust modelling; however, some work may be required to produce a similar formalism for computational responsibility.\par

Another effect of adopting a framework like Eigentrust for building a responsibility formalism would be that it would work significantly less well for modelling an agent's personal obligations, making it less suitable for parametrising the decision function of an intelligent agent. An argument to be made is that Eigentrust presents a way for an agent to assess its environment. This is true. However, consider that a public ledger where one agent was seen to be more responsible than another would lead all agents to assign tasks to that supremely responsible agent; tasks would not therefore be assigned to marginally less responsible agents without some significant work to make the global scores from an Eigentrust-based model suitable for a responsibility model. Using Eigentrust-based scoring systems would also limit one's ability to separate types of responsibility in a similar way to Marsh's separation of basic, general and situational trust: Eigentrust's very simple \(+/-1\) reputation system permits little insight into types of actions an agent is adept at.\par

Eigentrust presents an interesting and unique approach to trust formalism which is practical and effective. However, its limitations pertaining to the more social form of responsibility desired, combined with its higher degree of mathematical complexity over alternatives, means that it is more suitable as a curiosity than a foundation for the purposes of this work.\par

% --Omitted because I'm not yet read up enough on Gambetta.-- 
%\subsubsection{Gambetta}
%For the purposes of building a computatational formalism specifically, Gambetta's work is interesting mostly as a precursor to Marsh's. However, some concepts of Marsh's model are strikingly similar to Gambetta's.\par

%Gambetta's trade is ordinarily sociology. However, I include his work in this section because it is similar to Marsh's in its representation of values for its traits, which is important for Marsh's model's computational nature. 

\subsection{Ian Sommerville, Sociotechnical Systems, and Responsibility Modelling}
Sommerville's work focuses largely on sociotechnical systems and responsibility modelling --- in this way, Sommerville's work is not typically concerned with computational models of trust, as the above were. However, his work does begin to border on our own advancements, providing responsibility modelling formalisms.\par

% Formalisms generally graphical
It is important to note that Ian Sommerville has been a particularly prolific writer for a researcher in the sociotechnical systems scene. Sommerville's modelling systems are sometimes graphical\cite{sommerville_graphical_responsibility}. Unfortunately, graphical modelling systems do not lend themselves particularly well to computational formalism: they don't yield naturally to numerical analysis; they are generally designed for the purposes of human visual analysis, instead of logical reasoning; they are often difficult to represent non-graphically, which arguably makes input and manipulation too complex for the purposes of designing a complex intelligent agent around.\par

Ultimately, though, graphical responsibility modelling systems are designed for representing the responsibilities of a single agent at a given point in time; a responsibility formalism, by contrast, should be a series of metrics and rules which can apply to arbitrary reasoning of an agent's responsibility through time, with that agent using the formalism to reason about its changing responsibilities. In other words, graphical responsibility modelling differs from a responsibility formalism in that a responsibility formalism needs to \emph{generalise} reasoning about how responsibility, as a concept, ``behaves''.\par

% Sociotechnical perspective on responsibility
% Causal and Consequential responsibility --- just a brief outline, more when we discuss Scanlon in Philosophy section \cref{subsec:philosophy}
Nevertheless, some sociotechnical work on responsibility prevents an invaluable addition to relevant literature for developing its computational formalism. In particular is Sommerville's work on ``Causal'' and ``Consequential'' responsibilities. In defining these terms, Sommerville writes\cite{sommerville_dependable_systems_chap_8}:

\begin{displayquote}
    Consequential responsibility can only be assigned to a person, a role or an organisation – automated components cannot be blamed. Causal responsibility reflects who or what is responsible for making something happen or avoiding some undesirable system state. It is often the case that these are separated.
\end{displayquote}

The separation of concerns between consequential and causal responsibilities can help us to inform the structure and nature of a responsibility formalism. Particularly, one can simplify these definitions to be that:

\begin{displayquote}
    Consequential responsibilities are responsibilities which relate to a state arrived at in the past, and the relationship of actors and actions with said state.
\end{displayquote}\par

\begin{displayquote}
    Causal responsibilities are responsibilities which relate to future states, and the actors and actions which are potentially assigned with the goal of arriving at said state.
\end{displayquote}

This separation of past responsibilities and future responsibilities means that we can structure the concerns and operation of a responsible agent according to a given formalism. Particularly, for our purposes, the assessment of one's responsibility to arrive at future states --- one's assessment of its \emph{Causal Responsibilities} --- might be informed by information an agent's information about its previous responsibilities and its ability to act responsibly --- one's \emph{Consequential Responsibilities}. Therefore, we might decide to create a responsibility formalism which specifically models the change in causal responsibility, by assessing consequential responsibility.\par

As we approach a review of philosophical literature in \cref{subsec:philosophy}, we will find that a formalism geared toward causal responsibility is not only a very attractive way to model from a philosophical perspective, but that the philosophical literature on responsibility has converged on similar definitions of responsibility. Therefore, there appears to be a strong case that causal-first models --- aside from their simple structure and readiness to be tied into an agent's decision functions --- are a sound way of approaching the problem of framing problems concerning responsibility,  because of the consensus across fields which rarely interact.

% There's not much quantifiable on responsibility, but like Marsh, we can start from a non-quantified place and try to quantify it
% Philosophy and moral responsibility
\subsection{Philosophy}\label{subsec:philosophy}
Philosophy regarding moral responsibility is an area whose literature is both wide and deep. That said, not all moral responsibility literature is relevant to a computational responsibility project; lots of it is designed from a social analysis perspective which would be difficult to implement in any useful way. Other areas, however, present more promise for studies regarding formalisms.\par

\subsubsection{Peter F. Strawson}\label{sec:strawson}
One example of research with utility in a computational way is that of Peter F. Strawson, particularly in his seminal essay, Freedom and Resentment~\cite{strawson}. Strawson's topic actually revolves around whether determinism has any impact on ``free will'' --- a discussion clearly outside the scope of this project --- but in forming his argument creates some key concepts that we can use to consider the applicability of a responsibility formalism to a computational system, as well as touching on what that formalism would look like.\par

Strawson's fundamental argument can be construed as being that determinism doesn't affect what human factors --- like Responsibility and Trust --- mean, because these concepts are founded on the relationships between human actors, rather than being inherent to the human actors themselves. As computer scientists, we can extrapolate this argument out to a sociotechnical environment. That is: Strawson's argument applies to both social \emph{and} technological agents within a sociotechnical world. Using Strawson's reasoning, then, we can firmly conclude that it \emph{does} make sense to create ``responsible'' computers, because responsibility makes sense as a trait for a computer to have in the same way it might a human actor.\par

Strawson's insights don't stop there, though. He also produces an interestingly rigorous analysis of how ordinarily fuzzy human factors can be formalised appropriately:

\begin{displayquote}
    Indignation, disapprobation, like resentment, tend to inhibit or at least to limit our goodwill towards the object of these attitudes, tend to promote an at least partial and temporary withdrawal of goodwill; they do so in proportion as they are strong; and their strength is in general proportioned to what is felt to be the magnitude of the injury and to the degree to which the agent’s will is identified with, or indifferent to, it.
\end{displayquote}\cite{strawson}\par

Strawson captures an essential component of Marsh's computational trust model: an actor's actions influence the perceived trustworthiness of an agent in proportion of the magnitude of the ``trustworthiness'' of their actions. This concept, Strawson's elucidation shows, is one which can also extend to responsibility; it influences all of the human factors Strawson seeks to address in his essay (i.e.\ all human factors).\par

This gives us an insight into how a realistic computational trust model might function: its perception of responsibility should alter depending on external factors. We are aware, however, that people's actions are not the only things which can affect our feeling of responsibility: all sociotechnical factors might. For example, the ``Bystander Effect''\cite{unresponsive_bystander} occurs when one feels less responsible for acting in an emergency situation because of the number of people present who could help; in the end, nobody does, because every person's sense of responsibility is weakened. However, it is not weakened by any person's actions: it's influenced by a set of sociotechnical factors, of which the feelings that Strawson discusses are a subset.\par

A suitable conclusion one might draw, then, would be that a valid computational model of responsibility \emph{must} take into account an analysis of the sociotechnical environment of the responsible agent. It would also be valid to draw the conclusion --- as a result of the first of Strawson's points discussed --- that it makes sense to talk about ``trusting'' and ``responsible'' computational agents as if they were humans. Another important conclusion to draw from this part of Strawson's discussion is that this is one way in which agents' interaction is meaningful: therefore, there is a significant body of work to be undertaken in the combination of trust and responsibility formalisms to the field of HCI\@. Some of this work is already underway\cite{Marsh2011, Macy2002}. \par

Strawson also notes what sort of agent one rightly considers responsible. His argument relates to which agents one judges responsibility \emph{in}, rather than judging which actions an agent might consider itself responsible for. However, it is pertinent to this research in that it highlights what assumptions the formalism might make about an agent it models the responsibility of. In other words, Strawson helps us to limit the scope of the formalism in terms of the agents it targets.\par

\begin{displayquote}
    Let us consider, then, occasions for resentment \ldots{}To the first group belong [agents of whom we can say] ``He didn't mean to'', ``He hadn't realised'', ``He didn't know'' \ldots{}``He couldn't help it'' \ldots{}None of them invites us to suspend towards the agent, either at the time of his action or in general, our ordinary reactive attitudes.\\
    The second and more important subgroup of cases allows that the circumstances were normal, but presents the agent as psychologically abnormal—or as morally undeveloped. The agent was himself; but he is warped or deranged, neurotic or just a child. When we see someone in such a light as this, all our reactive attitudes tend to be profoundly modified.\\
\end{displayquote}\cite{strawson}

Strawson here demonstrates a useful separation of two groups of agents: those who incur resentment through a lack of control but who are fully able to act correctly, and those whose lack of control or basic understanding disqualifies them as agents who can incur resentment at all. A definition of resentment would be useful here; unhelpfully, Strawson doesn't lend one, but helpfully, a reasonable general definition of resent is easy to formulate: one feels resentment toward another agent when a goal entrusted to it is not achieved.\par

In other words, one feels resentment toward an agent which \emph{fails to fulfil a responsibility}. Using this definition, we can use Strawson's separation of resentment-inducing agents to limit the responsibility formalism's scope: an agent can be considered responsible for their actions when they can reasonably be assigned some goal (and possibly some actions to achieve this goal). Another way of saying this would be that an agent can be considered responsible for an action if we can \emph{trust} the agent with a goal; if a goal is assigned, then the act of assignment makes the agent responsible.\par

Using Strawson's reasoning regarding resentment, then, we can reasonably assert the implication: a responsible agent is an agent actively trusted with a task: a ``causal responsibility'', to borrow Sommerville's terminology. Along with the earlier notes on types of agents\cref{sec:agent_types}, we can even further limit the scope of agents the formalism should be targeted toward. We also neatly tie into our formalism the assignment of responsibility to an agent; more detail, along with what the assigned responsibility should represent, is discussed in the proposed approach\cref{sec:proposed_approach}.\par

\subsubsection{Thomas M. Scanlon}
In his essay, Justice, Responsibility, and the Demands of Equality\cite{scanlon2006justice}, Thomas Scanlon assesses responsibility as having two factors which bear striking resemblance to Sommerville's ``Causal'' and ``Consequential'' Responsibilities: ``Attributive'' and ``Substantive'' Responsibilities.\par

Scanlon discusses ``Attributive'' responsibilities as a group of duties according to the definition:
\begin{displayquote}
    What a person sees as a reason for acting, thinking, or feeling a certain way.
\end{displayquote}\par

For the purposes of later discussion, utility, and slight simplification, I will generalise Scanlon's definition to be ``responsibilities for future actions''. He also discusses ``Substantive'' responsibilities, which are loosely defined as responsibilities for the choices an agent has made, taking into account the effects they have and obligations at the time. We can generalise these to be ``responsibilities for past actions''. While in doing so, I reduce Scanlon's definitions to a discussion of action as opposed to the wider gamut of human properties, this framing sufficiently limits the scope of the work to apply elegantly to decision theory. Therefore, we can begin to apply Scanlon's work to the actions taken by responsible computational agents.\par

This limitation of scope also enables us to tie Sommerville's sociotechnical research to work done in the philosophical sphere. A complete responsible computational system, therefore, would be suitable for the same scrutiny that human agents currently undergo in the field of moral philosophy. It also, like Sommerville's work, presents us with a framework for thinking about the scope of responsibilities that an artificially intelligent agent might be subject to.\par

Beyond philosophical waxing lyrical, we have reason to limit our formalism to a subset of responsibilities Sommerville and Scanlon describe. Specifically, we can use Sommerville's ``Consequential'' or Scanlon's ``Substantive'' responsibilities to tailor a responsible agent's judgement of its ``Causal'' or ``Attributive'' responsibilities. This structure will act as a scaffolding for the simplified proposed framework in \cref{sec:work_plan}.

\subsubsection{Deontic Logic}
Deontic logic is a mathematical and philosophical logic of obligation.\cite{deontic-logic} It therefore would appear to lend itself perfectly to the task at hand; it would appear upon first glance that a responsibility formalism is already constructed. Unfortunately, this is not the case.\par

For one, deontic logic does deal with obligation, but from the perspective of formalising philosophical notions of action. Indeed, one of the insights a reader can expect to take from the work is a rigid definition of action. Deontic logic therefore deals with obliging to act as it contrasts to forbidding to act. Moreover, deontic logic creates relationships between different actions. For example: \[drive \Rightarrow O(park safely)\] is a deontic logic statement asserting that if one drives, one is obliged to park safely. However, the responsibility formalism required should assert a degree of responsibility for actions an agent might take at a given point in time --- this is required to answer the first research question. As a result, deontic logic is a fundamentally inappropriate platform for the work at hand to be built upon.\par

Another issue is that deontic logic is a logical framework, rather than a mathematical formalism. As a result, it is difficult to model gradations of levels of responsibility from the perspective of deontic logic without making significant changes to it. Were the theory of computational responsibility proposed more similar to Castelfranchi \& Falcone's model than Marsh's in its inspiration from computational trust formalisms, this would be less of an issue; however, Marsh's approach to his formalism suits our philosophy regarding computational responsibility much better.\par

\subsubsection{Sloman}\label{subsec:space-of-minds}
While this is already explored somewhat in the introduction to give context to this proposal, Sloman's work on the space of possible minds\cref{Sloman1984TheMinds} is an interesting example of how intelligent agents might be addressed as increasingly anthropomorphic ``minds''. Whether an intelligent agent can have what is termed a mind is a philosophical question beyond the scope of this proposal. Sloman's work, however, does lend itself some useful concepts with which one might consider anthropomorphic agents.\par 

Sloman's concept of the space of possible minds presents a notion that a mind, parametrised, can be represented by a series of spectra. These spectra combine as a mathematical Cartesian space, where a coordinate in that space represents a specific mind. There is, therefore, a subspace of this space which represents all possible human minds --- a larger subspace which represents all possible biological minds --- and possibly a broader still space which represents all possible minds, regardless of detail. Somewhere in this space, anthropomorphic algorithms reside.\par

The relevance and utility of Sloman's work to this responsibility formalism has already been presented: in acknowledging that an intelligent agent may not exist within the space of human minds, to impose responsibility upon the agent imposes human-like characteristics. In other words, the subspace of minds permissible by this formalism ought to be close to the subspace of human minds. To achieve this limitation, we impose upon the agents modelled by the formalism proposed traits of reflection, reaction, and interpretation.\par

\subsection{Discussion}
Computational Responsibility is a topic with a significant body of related literature, yet no specific literature on the topic. However, useful ideas expressed in the related literature --- from both backgrounds of the humanities and the sciences --- help to shine light on a path toward a formalism which will help to answer the research questions posited in \cref{sec:statement_of_problem}.\par

It's worth noting that Philosophy and Computing Science are unusual bedfellows. However, the deeply philosophical nature of this work shows that this need not be the case. Philosophical research may well have an impact on the ethics and cultural implications that advances in Computing Science make; it is important therefore to promote where possible interdisciplinary work of this nature. The philosophical literature reviewed is of great help in directing the project's next steps.\par

A formalism which does help to answer the research questions introduced will have to:

\begin{itemize}
    \item Provide gradations of responsibility measures, in a similar way to that which Marsh's formalism permits
    \item Allow for the updating of the interpretation of responsibilities in a similar way to that which Strawson suggests
    \item Primarily address Sommerville's ``Causal'' responsibilities, or (the given interpretation of) Scanlon's ``Attributive'' responsibilities
    \item They both follow Strawson's note on how agents with human factors change their outlook with respect to these factors through interaction
    \item Account for responsibility on the level of personal responsibility and societal responsibility\\
        (This would be achieved by agents who act on individual responsibility but who as a collective use responsibility to lower negative exposure, as per Luhmann's notes on risk)
\end{itemize}

In the proposed approach in the following section, we will explore possible formalisms of responsibility which leverage ideas from all of these useful insights from related literature, in order to better answer the research questions laid out. This will involve adopting a Marsh-like foundation for the formalism, and using interpretation functions and similar devices, with mathematical formalisms in the spirit of Birkhoff's work on aesthetics. Its behaviour and approach to responsibility will be informed and directed by the relevant aspects of Strawson and Scanlon's writing.\par

Eigentrust, as a foundation for this model, was unfortunately unsuitable. However, an open possibility is to adopt a similar reputation system for assessing the responsibleness of other agents --- this is unexplored in \cref{sec:proposed_approach}, as it would not fit with the current conception of Marsh-like responsibility. Should such an approach be required, this will be assessed as the formalism is completed. The roadmap to completing the formalism is explored in \cref{sec:work_plan}

% --Also worth mentioning that responsibility's not as useful for risk analysis as trust is.--

% Comparing early trust and early responsibility: psychology/sociology of trust and philosophy of responsibility
\subsubsection{The Relationship Between Trust and Responsibility}
Trust and responsibility are similar concepts. Seen through the lens of C\&F, this is particularly clear: 

\begin{itemize}
    \item They both concern themselves with actions and goals
    \item They're both naturally framed in terms of task delegation
\end{itemize}

Trust and Responsibility's similarity regarding task delegation is particularly interesting: this might allow a responsibility formalism to operate in a similar way to trust, meaning that much of the existing literature on Trust could be re-appropriated (with some research) for the purposes of computational responsibility. Indeed, one definition of responsibility for a task might be the obligation to act on a delegated task. A corollary of C\&F argue that task delegation is inherently trust; then, all one need do to extend a trust formalism to a responsibility formalism would be to augment C\&F's axioms to include an agent's obligation, and one's formalism would be complete!\par

Unfortunately, C\&F's formalism, while technically calculable by a computer, uses logical expressions to evaluate trust. For the purposes of an application such as a decision function of an intelligent agent, unless that agent follows a structure such as a BDI model, this would not be terribly useful --- though it's worth noting that granular models of C\&F also exist\cite{lorini2008binary}.

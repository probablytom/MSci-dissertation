\documentclass{mprop}

% alternative font if you prefer
\usepackage{palatino}

% for alternative page numbering use the following package
% and see documentation for commands
%\usepackage{fancyheadings}
\usepackage{csquotes}


% other potentially useful packages
%\uspackage{amssymb,amsmath}
%\usepackage{url}
%\usepackage{fancyvrb}
%\usepackage[final]{pdfpages}

% Packages for image layouts
\usepackage{graphicx}
\usepackage{float}

\newcommand{\safealpha}{\(\alpha\)}

% A command to create red TODO markings.
\usepackage{color}
\newcommand{\todo}[1]{}
\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}

\usepackage{enumitem}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Proposing a Model of Computational Responsibility}
\author{William T. Wallis}
\date{}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{intro}

Computational Responsibility is a field with little to no existing literature. Rather than a focus on \emph{responsibility}, researchers have so far tackled a variety of other social topics through computational formalisation:

\begin{itemize}
    \item Marsh's seminal work on Trust\cite{Marsh1994FormalisingConcept}
    
    \item Stricter formal definitions on Trust, from a cognitive standpoint\cite{CastelfranchiSocialApproach}
    
    \item Some responsibility modelling, from a logical formalisation\cite{Simpson2015FormalisingAnalysis} \emph{I'm still reading this, Tim!}
    
    \item Some work on reputation \cite{Chandrasekaran2011ASystems}
    
\end{itemize}

something something there's a gap here where nobody's applied machine learning to the problem for teaching artificial agents about the social concept of responsibility


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Statement of Problem}

Computational responsibility is a complex area with lots of incidentally related work, but no specific relevant literature. Instead of focusing on the responsibilities of artificial agents, their responsibilities are implied by the construction of the agent itself. It might employ algorithms for driving without human guidance, or classify network traffic in an attempt to flag attempts at a system's security. In these instances, lots of somewhat-related work has been done on computational \emph{trust}: can one artificial agent trust another?\par

However, this approach is short-sighted. While trust and responsibility are intrinsically linked social concepts, no work has been done to migrate the models of trust to new models of responsibility. A concern arises: do artificially intelligent agents, which we put at the helm of concerns like network security and road safety, actually communicate its understanding of its assigned duty with other agents it collaborates with? Two examples present themselves.\par

The first: a car might drive along a residential street and identify a squirrel running across the road in front of it. It calculates a high probability that, unless it swerves out of the way of the squirrel, it may kill it. It simultaneously identifies that, in the country it is driving in, the law states that it should swerve to avoid killing animals if possible. Computational responsibility introduces itself into the problem in that the car should also have a social understanding: will the swerve endanger humans? How strongly should it weight that probability into the action it chooses? Is it also responsible for, say, conserving fuel for environmental reasons? The key here is that the car has many goals to ascertain; while some are more immediate than others, it should have the capacity to weigh \emph{multiple, arbitrary responsibilities} up to surmise what its next action is. \par

The second: an artificially intelligent agent watches the price of a collection of books in an online store. This is common practice on large sites where prices of unusual books can fluctuate wildly. 

\begin{figure}[ht]
\centering
\includegraphics[trim=0.7cm 0.7cm 0.7cm 0.7cm, scale=0.75]{images/amazon_price_hike_fly_genetics}
\caption{Bots on Amazon artificially inflate a book price to around \emph{62850\%} its used price}
\end{figure}

Here one artificial agent is known to have artificially inflated the price of a book; another agent has \emph{also} inflated the price according to the seeming market trend. The first agent, seeing that the book is rising in value and now underpriced, inflates the price of its own copy, and the cycle continues until a human intervenes. \par

Kevin Slavin discusses the idea that we have begun to design a world \emph{"for algorithms\cite{SlavinHOWWORLD}, with nothing but a big red button, labelled 'stop'"}. The precession of this design trend marches on, relentless --- but algorithms, rather than their interfaces, can be built with humans in mind. A mutual understanding of responsibility would allow one algorithm in this cycle to delegate the price inflation of its book to the other, breaking the cycle, so long as the concept of responsibility for a task is mutually understood. This is where the second, real-world example of computational responsibility lies. \par

As can be seen in the model proposed by Castelfranchi \& Falcone in their formulation of cognitive trust\cite{CastelfranchiSocialApproach} (usually referred to as \emph{C\&F Theory}), a formulation of trust surrounding one or more actors, subjectively assessing tasks and goals, which takes into account social and technical factors in its modelling, is already present. Fortunately, this model is very well accepted by the computational trust community! Therefore, some work presents itself: does an adaptation of C\&F theory suit a practical model and implementation of computational responsibility? Secondly, one is also led to wonder: how well would such a model solve the example applications of computational responsibility explained earlier?


\subsection{A need for a cognitive computational responsibility}

As we move into a world increasingly dominated by algorithms and shaped by their decisions, there is a clear requirement for responsible systems. One problem arises: how can we be certain that an algorithm's internal conception of responsibility is "human-like"? Early work by Sloman describes the notion of a "space" of minds\cite{Sloman1984TheMinds}, and this concept is useful here. An artificial mind need not be human-like, or even biological-like; it can occupy an entirely different area of the space of minds altogether. \par

This is problematic when imposing human-like concepts onto machines. The effort of imposing a social, human-like construct onto a "mind" that may not easily host the concept means that one runs the risk of imitating the trait in a useful way, but not in an accurate way. If the model of the human trait doesn't stem from the same basic concepts as the human model does, it cannot be relied on to behave in a human-like way all of the time. \par

Therefore, while we have already demonstrated a need for computational responsibility, there is another requirement that must be satisfied: \emph{cognitive} computational responsibility. \par

C\&F define a cognitive agent as:
\begin{displayquote}
``Only a cognitive agent can ``trust'' another agent; only an agent \emph{endowed with goals and beliefs}''
\end{displayquote}\par

This definition doesn't quite fit our purposes --- as will be seen, our definition also requires the concept of \emph{obligation}. However, it can be seen that this definition is deliberately high-level in order to simulate the important components of a human trusting agent. A cognitive agent can be seen as an agent which, for the task it is set out to do, is modelled in a \emph{high-level, human like way}. \par

Thus, a cognitive model of responsibility is necessary; an ordinary model might suffice for theoretical or research purposes, and may be useful in analysing scenarios bound tighter by, say, law, than they are by the normal human's cognition.\par


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background Survey}

% What work's been done already? 
Unlike Responsibility, Trust is a topic which has a surprising degree of pre-existing literature. Marsh \cite{Marsh1994FormalisingConcet} draws inspiration from as early as David Birkhoff's 1930s work in creating an "Aesthetic Measure", where the famous mathematician created a quantification of Aesthetics. While some dispute that such subjective topics can be boiled down to a single number (or array thereof), much work to the contrary has now been completed. Like Marsh, we should start from the beginning. 

% Maths, Psychology and Sociology a-la Marsh
\subsection{Social Sciences and Mathematics}
\subsubsection{Birkhoff's Aesthetic Measure}
One of the earlier formalisms of a human factor\footnote{For the sake of clarification, we define a "human factor" as an element of a social or sociotechnical system which arises from human behaviour, such as Trust.}\footnote{Also for the sake of clarifying a sociotechnical system, a sociotechnical system is a system composed of human tendencies and behaviours, such as Trust, alongside technical activity, such as a computer or a steam engine. An example might be a coffee shop: \begin{itemize}
    \item Humans take orders and manage the running of the shop
    \item Technology is responsible for complex activities such as taking payments and forcing steam through coffee at high pressure
\end{itemize} so there are both social and technical actors and behaviours in the "system" of a day-to-day coffee shop.} was Birkhoff's definition of Aesthetic Measure\cite{BirkhoffAESTHETICMEASURE}. In it, Birkhoff defines the notion of Aesthetic Measure as a ratio of Order to Complexity:
\[M = \frac{O}{C}\]
Birkhoff's work inadvertently gave rise to the notion that human factors can be represented by mathematical equations and systems. Birkhoff's formalism of aesthetics became popular for a few reasons, but one of particular interest to later Trust modelling work was that Birkhoff put a great degree of effort into backing his work up with psychological theory. In this way, Birkhoff's formalism could be said to be a \emph{psychological} formalism. \par

Later trust modelling work followed in Birkhoff's footsteps here. Indeed, Birkhoff gives a solid foundation for the model-creating method later employed by Marsh\cite{Marsh1994FormalisingConcept} and Castelfranchi \& Falcone, as it is:
\begin{itemize}
    \item Founded on mathematical or logical principles which are \emph{quantifiable}
    \item Heavily inspired and directed by related work in psychology, sociology, and philosophy
\end{itemize}\par

The marriage of social studies with mathematical rigour will be a recurring theme of the work related to Computational Trust.\par

\subsubsection{Deutsch}
Following the quantifiable, mathematical work done by Birkhoff, logical and arithmetic formalisms of human factors followed. One of the earlier and more widely adopted models for Trust came from Deutsch in 1962. 
Deutsch is a psychologist who did swathes of work in the topic of cooperation, touching on Trust during the 60s. \par

Deutsch's formalism of trust wasn't immediately quantifiable, but presented one of the earliest well-defined definitions of trust. To paraphrase Deutsch's formalism in "Cooperation and Trust: Some Theoretical Notes", 1962\todo{CITE THIS}:
\begin{itemize}
    \item An actor is presented with a choice between two paths.
    \begin{enumerate}[label=\emph{\Alph*}:]
        \item No change
        \item The actor takes some action, of ambiguous outcome. A possible gain is associated, \emph{P}, and some possible risk is associated, \emph{R}.
    \end{enumerate}
    \item The actor assesses that the outcome of choice \emph{B} relies on the behaviour of another actor.
    \item The actor assesses the action they may take and resolves that the strength of \emph{R}, likelihood of \emph{R} as an outcome, or both are higher than the respective \emph{P} measurements.
    \item The actor is said to be \emph{trusting} they take path \emph{B}.
\end{itemize}

This formalism introduces some interesting notions. For example, it is unclear as to whether the outcome of choice \emph{B} can rely on the same actor making the decision; can one trust oneself by Deutsch's definition? Another interesting analysis of the implications of Deutsch's model is that it does not rely on the \emph{accurate} measurement of risk and utility, but just its perception -- trust is subjective, and based on the trusting actor's perspective on the world.

Rather than characterising trust by the parties involved, Deutsch's formalism is characterised by \emph{risk and utility}. A simple quantification of Deutsch's formalism could be devised, therefore, where risk and utility are quantified by simple assessments using utility functions and a form of risk analysis. Even so, the outcome of this quantified system is a single bit: trusting or not trusting. This does quantify trust, but only technically speaking, and this quantification is weak in its expressiveness. It gives no remit to suggest that one might trust one person over another, for example, as there are no orderable degrees of trust.

Deutsch offers many different ideas as to why and how trust or trust-like behaviour can come about, however. This list is taken from Marsh 1994\cite{Marsh1994FormalisingConcept}, where explanations of all nine can be found:

\begin{enumerate}
    \item Trust as Despair
    \item Trust as Social Confirmity
    \item Trust as Innocence
    \item Trust as Impulsiveness
    \item Trust as Virtue
    \item Trust as Masochism
    \item Trust as Faith
    \item Risk-taking or Gambling
    \item Trust as Confidence
\end{enumerate}

Deutsch's given model above specifically targets formalisation of trust as confidence.

\subsubsection{Luhmann}
Luhmann, a sociologist who also worked in Trust and related fields, had his own take on formalisms of Trust: that trust was a concept which 


% Modern trust (logics, Marsh and C&F)
\subsection{Modern [Computational] Trust methods}


% There's not much on responsibility, but like Marsh, we can start from a non-quantified place and try to quantify it
% Philosophy and moral responsibility
\subsection{Philosophy of Moral Responsibility}


% Comparing early trust and early responsibility: psychology/sociology of trust and philosophy of responsibility
\subsection{Comparing Trust and Responsibility}


% Follow up question: what work hasn't been done yet?
% Responsibility work that we can do given the comparison between early trust and early responsibility
\subsection{What work is missing?}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Approach}

\subsection{C\&F: Close, but no cigar}
As it turns out, cognitive computational trust models that already exist are almost but not quite appropriate for modelling responsibility, too. The C\&F trust model requires only four main ingredients to formulate a cognitive trust model:

\begin{enumerate}
    \item \emph{x}, a truster
    \item \emph{y}, a subject of trust
    \item \emph{g}, a goal of \emph{x}
    \item \emph{\safealpha}, an action of \emph{y}
\end{enumerate}\par

This model gets us close to where we need to be to model responsibility; like responsibility modelling often does, it assumes two agents. There also exists some goal which can be met, which --- to use C\&F terminology --- is \emph{delegated} by \emph{x} to \emph{y}. \emph{Y} can achieve this goal through some action, \emph{\safealpha}. So far, all of this forms the beginning of a foundation for cognitive responsibility; what turns delegation of a task into the consignment of responsibility is that of obligation, and the understanding of obligation. \par

It is evident that trust and responsibility models are, even in the human-like cognitive approach, very similar. However, crucial differences mean that we cannot directly apply C\&F theory to the idea of computational responsibility. \par

Therefore, I propose that research must be carried out to ascertain whether C\&F can, as a model, be adapted simply to account for an agent's responsibility. In addition, research must be carried out to implement this model in a BDI logic agent, enabling the evaluation of the new model's success. \todo{WE SHOULD BE ABLE TO ADAPT \cite{HubnerFromTrust} TO IMPLEMENT OUR NEW COGNITIVE RESPONSIBILITY MODEL IN A BELIEF, DESIRE, INTENTION AGENT MODEL. THEY DO A DIRECT APPLICATION OF C\&F TO BDI, WE SHOULD TOO.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Work Plan}

Panic, write the report in a 36 hour caffeine-induced fever dream

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% it is fine to change the bibliography style if you want
\bibliography{mendeley}
\bibliographystyle{plain}
\end{document}
